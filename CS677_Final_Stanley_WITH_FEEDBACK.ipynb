{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08546210",
   "metadata": {},
   "source": [
    "# CS 677 Final Project – NYC Census Data with SVM (Fixed Version)\n",
    "\n",
    "This notebook is the **corrected full version** with:\n",
    "\n",
    "- Proper handling of missing values  \n",
    "- One-hot encoding for **Borough** and **County**  \n",
    "- Outlier inspection  \n",
    "- Train/test split with stratification  \n",
    "- Standardization (scaling)  \n",
    "- PCA visualization  \n",
    "- Multiple ML models: SVM (linear & RBF), Logistic Regression, SGD (gradient descent)  \n",
    "- Hyperparameter tuning with GridSearchCV  \n",
    "- Evaluation metrics (Accuracy, Precision, Recall, F1)  \n",
    "- Confusion matrix, ROC curves  \n",
    "- Learning curve and error vs training size  \n",
    "- Final model comparison table  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df72db9b",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "The goal of this project is to predict whether a New York City census tract is **high income** or **low income** using demographic, socioeconomic, and commuting-related features from the NYC Census Tracts dataset. \n",
    "\n",
    "This is a **binary classification** task. We train and compare several models covered in CS 677 – a linear Support Vector Machine (SVM), a non‑linear SVM with RBF kernel, Logistic Regression, and an SGD‑based linear classifier – and evaluate them using accuracy, precision, recall, F1‑score, confusion matrices, ROC curves, and learning curves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1032d8",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6e989e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_curve, auc\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21a524f",
   "metadata": {},
   "source": [
    "## 2. Load Dataset & Initial Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4993a467",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load NYC census tracts dataset\n",
    "# Make sure `nyc_census_tracts.csv` is in the same folder as this notebook.\n",
    "df = pd.read_csv(\"nyc_census_tracts.csv\")\n",
    "\n",
    "# Drop rows where Income is missing, because the target HighIncome depends on this column\n",
    "df = df.dropna(subset=[\"Income\"])\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cefd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383483b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.describe().T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c98650b",
   "metadata": {},
   "source": [
    "### 2.1 Missing Values – Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c91a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Count missing values per column\n",
    "missing_counts = df.isnull().sum().sort_values(ascending=False)\n",
    "missing_counts.head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ac371b",
   "metadata": {},
   "source": [
    "We handle missing values using two different strategies:\n",
    "\n",
    "- **Numeric columns** are filled with their **median**. The median is robust to skewed distributions and outliers, which is common in socioeconomic variables such as income, poverty, or unemployment.\n",
    "- **Categorical columns** (`Borough`, `County`) are filled with their **mode** (most frequent value). This preserves existing categories without creating artificial new labels.\n",
    "\n",
    "After imputation, the dataset has no missing values and is ready for feature engineering and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0444bb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "categorical_cols = ['Borough', 'County']\n",
    "\n",
    "# Impute numeric columns with median\n",
    "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "\n",
    "# Impute categorical columns with mode (most frequent value)\n",
    "for col in categorical_cols:\n",
    "    df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "df.isnull().sum().sort_values(ascending=False).head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c98f68b",
   "metadata": {},
   "source": [
    "### 2.2 Outlier Inspection – Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadaa0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "sns.boxplot(x=df['Income'])\n",
    "plt.title(\"Boxplot of Income\")\n",
    "plt.show()\n",
    "\n",
    "Q1 = df['Income'].quantile(0.25)\n",
    "Q3 = df['Income'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = df[(df['Income'] < lower_bound) | (df['Income'] > upper_bound)]\n",
    "len(outliers), lower_bound, upper_bound\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5283ab",
   "metadata": {},
   "source": [
    "We inspect potential outliers using boxplots.\n",
    "\n",
    "- For **Income**, there are several very high‑income tracts that appear as extreme values.\n",
    "- We also examine additional economic variables such as `Poverty`, `ChildPoverty`, `IncomePerCap`, and `Unemployment`.\n",
    "\n",
    "These extreme values correspond to real neighborhoods (for example, very wealthy areas or very disadvantaged tracts), so they carry important information about NYC.  \n",
    "Therefore, instead of removing them, we **keep these outliers** in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a4d7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Explore outliers in a few additional economic variables\n",
    "outlier_cols = ['Poverty', 'ChildPoverty', 'IncomePerCap', 'Unemployment']\n",
    "for col in outlier_cols:\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    sns.boxplot(x=df[col])\n",
    "    plt.title(f\"Boxplot of {col}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e852bf3",
   "metadata": {},
   "source": [
    "### 2.3 Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6749d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df.select_dtypes(include=[np.number]).corr(), cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Heatmap (Numeric Features)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3415f2cb",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering – HighIncome Target & Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1cc178",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create binary HighIncome target based on median Income\n",
    "median_income = df['Income'].median()\n",
    "df['HighIncome'] = (df['Income'] > median_income).astype(int)\n",
    "df['HighIncome'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad9e69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# One-hot encode Borough & County\n",
    "df = pd.get_dummies(df, columns=['Borough', 'County'], drop_first=True)\n",
    "\n",
    "# Drop ID-like and leakage columns\n",
    "drop_cols = ['CensusTract', 'Income', 'IncomeErr', 'IncomePerCap', 'IncomePerCapErr']\n",
    "existing_drop = [c for c in drop_cols if c in df.columns]\n",
    "df = df.drop(columns=existing_drop)\n",
    "\n",
    "# Define X and y\n",
    "X = df.drop(columns=['HighIncome'])\n",
    "y = df['HighIncome']\n",
    "\n",
    "X.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3ed80b",
   "metadata": {},
   "source": [
    "## 4. Train/Test Split & Scaling\n",
    "\n",
    "We split the data into **training (80%)** and **test (20%)** sets using stratified sampling so that the proportion of high‑income vs low‑income tracts is preserved.\n",
    "\n",
    "Before training SVMs and Logistic Regression, we apply **StandardScaler** to all features.  \n",
    "These models are sensitive to the scale of the input variables: if one feature has much larger numeric values than others, it can dominate the decision boundary.  \n",
    "Standardization (zero mean, unit variance) ensures that:\n",
    "\n",
    "- All features contribute on a similar scale.\n",
    "- Gradient‑based optimization converges more reliably.\n",
    "- The SVM margin is not biased by units of measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b43fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split into train and test sets (stratify to keep HighIncome ratio similar)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Standardize features for SVM / Logistic Regression\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled.shape, X_test_scaled.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f1c23e",
   "metadata": {},
   "source": [
    "### 4.1 PCA Visualization\n",
    "\n",
    "To better understand whether the two classes are separable, we project the standardized features into **two principal components (PC1 and PC2)** using PCA and color the points by income class.  \n",
    "Although there is some overlap, high‑income and low‑income tracts form partially distinct clusters, confirming that the features contain signal for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee277597",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reduce scaled features to 2D with PCA for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "scatter = plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1],\n",
    "                      c=y_train, cmap='coolwarm', alpha=0.7)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"PCA Projection of Training Data\")\n",
    "\n",
    "# Add legend for income classes\n",
    "import matplotlib.patches as mpatches\n",
    "low_patch = mpatches.Patch(color=plt.cm.coolwarm(0.1), label='Low Income')\n",
    "high_patch = mpatches.Patch(color=plt.cm.coolwarm(0.9), label='High Income')\n",
    "plt.legend(handles=[low_patch, high_patch], title=\"Income Class\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3566ce5a",
   "metadata": {},
   "source": [
    "## 5. Model Training – SVM, Logistic Regression, SGD\n",
    "\n",
    "We train four classification models that correspond to topics covered in CS 677:\n",
    "\n",
    "1. **SVM (Linear kernel)** – learns a linear decision boundary that maximizes the margin between high‑income and low‑income tracts.\n",
    "2. **SVM (RBF kernel)** – uses the radial basis function kernel to create a **non‑linear** decision boundary in the original feature space. This allows the model to capture more complex relationships using the kernel trick.\n",
    "3. **Logistic Regression** – a probabilistic linear classifier that serves as a strong baseline and is easy to interpret.\n",
    "4. **SGDClassifier (hinge loss)** – an approximate linear SVM trained with **stochastic gradient descent**. It is efficient on large datasets and demonstrates the optimization techniques studied in class.\n",
    "\n",
    "All models are trained on the standardized features (`X_train_scaled`) and evaluated on the standardized test set (`X_test_scaled`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea28a855",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train linear SVM\n",
    "svm_linear = SVC(kernel='linear', probability=True, random_state=42)\n",
    "svm_linear.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Train non‑linear SVM with RBF kernel\n",
    "svm_rbf = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "svm_rbf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Train Logistic Regression baseline\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Train linear SVM using SGD (hinge loss)\n",
    "sgd_clf = SGDClassifier(loss='hinge', max_iter=1000, random_state=42)\n",
    "sgd_clf.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e32833",
   "metadata": {},
   "source": [
    "### 5.1 Evaluation Helper & Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43307935",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper function to compute classification metrics for a given model\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    return {\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"Precision\": precision_score(y_test, y_pred),\n",
    "        \"Recall\": recall_score(y_test, y_pred),\n",
    "        \"F1 Score\": f1_score(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "# Evaluate all models on the scaled test set\n",
    "results = {\n",
    "    \"SVM Linear\": evaluate_model(svm_linear, X_test_scaled, y_test),\n",
    "    \"SVM RBF\": evaluate_model(svm_rbf, X_test_scaled, y_test),\n",
    "    \"Logistic Regression\": evaluate_model(log_reg, X_test_scaled, y_test),\n",
    "    \"SGDClassifier\": evaluate_model(sgd_clf, X_test_scaled, y_test)\n",
    "}\n",
    "\n",
    "pd.DataFrame(results).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921009d9",
   "metadata": {},
   "source": [
    "### 5.2 Confusion Matrices – All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7110fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize confusion matrices for all four models on the same figure\n",
    "model_dict = {\n",
    "    \"SVM Linear\": svm_linear,\n",
    "    \"SVM RBF\": svm_rbf,\n",
    "    \"Logistic Regression\": log_reg,\n",
    "    \"SGDClassifier\": sgd_clf\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "for i, (name, model) in enumerate(model_dict.items(), start=1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    cm = confusion_matrix(y_test, model.predict(X_test_scaled))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(name)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ae2273",
   "metadata": {},
   "source": [
    "### 5.3 ROC Curves – All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4024527",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot ROC curves for all models\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "for name, model in model_dict.items():\n",
    "    y_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC={roc_auc:.3f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curves – All Models\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da4cfde",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Tuning – SVM RBF\n",
    "\n",
    "To improve performance of the non‑linear SVM, we tune the **C** (regularization strength) and **gamma** (RBF kernel width) hyperparameters using `GridSearchCV` with 5‑fold cross‑validation.  \n",
    "This searches over a small grid of candidate values and selects the combination that maximizes cross‑validated accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4a08c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_grid = {\n",
    "    \"C\": [0.1, 1, 10],\n",
    "    \"gamma\": [\"scale\", \"auto\", 0.01, 0.001],\n",
    "    \"kernel\": [\"rbf\"]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    SVC(probability=True, random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "grid_search.best_params_, grid_search.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0b340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_svm = grid_search.best_estimator_\n",
    "best_results = evaluate_model(best_svm, X_test_scaled, y_test)\n",
    "best_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6f7756",
   "metadata": {},
   "source": [
    "## 7. Learning Curve – SVM RBF\n",
    "\n",
    "We use a learning curve to study how the RBF SVM behaves as we increase the amount of training data.  \n",
    "The plot shows training and validation accuracy for different training set sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a893cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    svm_rbf,\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    cv=5,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "train_mean = train_scores.mean(axis=1)\n",
    "test_mean = test_scores.mean(axis=1)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(train_sizes, train_mean, marker=\"o\", label=\"Training Accuracy\")\n",
    "plt.plot(train_sizes, test_mean, marker=\"o\", label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Training Set Size\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Learning Curve – SVM RBF\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3455087",
   "metadata": {},
   "source": [
    "### Interpretation of the Learning Curve\n",
    "\n",
    "- **Training accuracy** starts very high for small training sizes and decreases slightly as we add more data.  \n",
    "- **Validation accuracy** improves as the training set grows and then stabilizes around a similar range as training accuracy.\n",
    "\n",
    "This pattern suggests that the RBF SVM benefits from more data and ends up with **low variance and low bias**:  \n",
    "the model is not severely overfitting (training and validation curves are close) and additional data provides diminishing returns after a certain point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f311d546",
   "metadata": {},
   "source": [
    "## 8. Final Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ea0d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combine baseline model metrics with tuned SVM results in a single table\n",
    "all_results = pd.DataFrame(results).T\n",
    "all_results.loc[\"Best SVM (Tuned)\"] = best_results\n",
    "all_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae554e27",
   "metadata": {},
   "source": [
    "### Discussion & Conclusion\n",
    "\n",
    "From the final comparison table we observe that:\n",
    "\n",
    "- All four models achieve **strong performance** with accuracies around the high‑80% range.\n",
    "- The **tuned SVM with RBF kernel** achieves the best overall balance of metrics (highest or near‑highest accuracy, precision, recall, and F1‑score).\n",
    "- The **linear models** (Linear SVM, Logistic Regression, SGDClassifier) perform only slightly worse, which suggests that much of the class separation is close to linear in the feature space.\n",
    "- **SGDClassifier** is somewhat behind the other models but is still competitive and demonstrates how stochastic gradient descent can approximate a linear SVM efficiently.\n",
    "\n",
    "Overall, we conclude that:\n",
    "\n",
    "1. Demographic and socioeconomic features from the NYC census tracts contain enough signal to reliably distinguish **high‑income vs low‑income** neighborhoods.  \n",
    "2. Non‑linear SVM with RBF kernel is a strong choice for this problem, especially after hyperparameter tuning and proper scaling.  \n",
    "3. For deployment in a real system, a simpler linear classifier (e.g., Logistic Regression or Linear SVM) could be preferred if interpretability and training speed are more important than the last few percentage points of accuracy."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
